{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52815b5d",
   "metadata": {},
   "source": [
    "# SI-SI5  Lab4 : text classification, machine translation\n",
    "\n",
    "## Work to do and assessment policy:\n",
    "\n",
    "- Work by pairs, according to the table sent by email\n",
    "- The two parts A and B of this lab are independent.\n",
    "- You are only requested to do part A to fully validate your grade\n",
    "- Part B comes as bonus points, as your mark will be computed as \n",
    "$$\n",
    "mark = \\min(20, part_A + \\frac{1}{2} part_B)\n",
    "$$\n",
    "- Fill this notebook and drop it on https://mvproxy.esiee.fr no later than January 21th 2024, 23:59\n",
    "- <b>No submission by email</b>. Submissions by email will not be evaluated.\n",
    "\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Terminal setup (recommended)\n",
    "\n",
    "This lab has been validated on a Docker container, on which a generic CPython version of Tensorflow has been installed. To run the container on ESIEE's machines, booted as Linux :\n",
    "\n",
    "```curl -k https://mvproxy.esiee.fr/tmp/nlp.docker.tar | docker load```\n",
    "\n",
    "```docker run -it -u user fedora:38 bash```\n",
    "\n",
    "The first line should take a while (about a minute) while remaining silent, just be patient. Once you are logged on the container, launch ```cd ~```, then ```python3```. You can copy and paste the snippets below to your Python interpreter, and update this nootebook localky. \n",
    "\n",
    "### 1.2 VNC setup\n",
    "\n",
    "Alternatively, if you want to do everything on the container thanks to VNC, do the following. On the docker, launch\n",
    "\n",
    "```sudo vncsession user :1```\n",
    "\n",
    "```ifconfig```\n",
    "\n",
    "The last line should show a network interface with an IP of the form 172.17.0.2\n",
    "Then, on your host machine:\n",
    "\n",
    "```xtigervncviewer 172.17.0.2:1```\n",
    "\n",
    "You should have the usual 'fluxbox' desktop. \n",
    "\n",
    "### 1.3 Sharing files with the container\n",
    "\n",
    "If you wish to share your ESIEE's home directory within the container, you have two options:\n",
    "\n",
    "Option 1: replace ```login``` by your ESIEE's login in the following command line:\n",
    "\n",
    "```sshfs -o allow_other -o uid=1000 -o gid=1000 login@172.17.0.1:\\~ /mnt/esiee```\n",
    "\n",
    "Option 2 : on ESIEE's machine, run \n",
    "```chmod go+rx $HOME```\n",
    "\n",
    "Then launch the container with the -v option, something like:\n",
    "```docker run -it -u user -v$HOME:/mnt/esiee fedora:38 bash```\n",
    "\n",
    "No matter which option you prefer, you should always find your ESIEE's files in ```/mnt/esiee``` within the container. Beware, however, that with option 2, the user under which the container is ran has  an UID >= 65000 and is neither you nor your ESIEE's group, so permissions of your file should be granted to *others*\n",
    "\n",
    "\n",
    "## Part A : text classification\n",
    "\n",
    "In this part, you will have to finish the implementations of two RNN-based models shown on slides 28 and 38 of [Chapter 4](https://perso.esiee.fr/~hilairex/5I-SI5/rnn.pdf). Both networks accept words as input, from sentences which don't exceed a certain length, and aim to perform text classification. \n",
    "\n",
    "You will work on the the IMDB reviews dataset, hosted by Kaggle [here](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download), which you should download and extract in your working directory -- only a single file 'IMDB Dataset.csv' is needed\n",
    "\n",
    "The following code snippets perform the first steps on text for you - loading, vectorising, and training a basic (non-recurrent) FFN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec980dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\langage-naturel-esiee\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "reviews = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed6c50",
   "metadata": {},
   "source": [
    "We first perform a standard test/train split. During development, I strongly suggest that you first use a small amount of samples (1000) for validation. IMDb has 50000 reviews, which is too much. Keep in ming that training RNNs is *slow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e7f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_test_split(reviews, shuffle=True, train_size=1000, test_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55439f5d",
   "metadata": {},
   "source": [
    "The next step is to vectorize the text. In Lab3, I provided a vecto() function which did this, with relevant padding. I also mentioned Keras offered a TextVectorization layer which did exactly the same job. Its effects are shown below. \n",
    "\n",
    "In particular, note that unknown words yield an index of 1, and 0 is used for padding. So real indexation starts at index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c57ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\langage-naturel-esiee\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\langage-naturel-esiee\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[ 8, 10,  2,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  7,  1,  0,  0,  0,  0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text vectorization : quick demo\n",
    "vecto= tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=99, output_mode='int', output_sequence_length=10)\n",
    "vecto.adapt([[\"I am the king of the world\"],[\"You are the queen\"]])\n",
    "vecto([[\"I am the queen\"],[\"World is king unknown\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef63e2e",
   "metadata": {},
   "source": [
    "We now change the call to adapt the layer to our train data. Note that IMDb reviews are rather long (about 300 words / review on average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61771054",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words=3000  # the vocabulary size\n",
    "seq_len=300     # maximum sequence length\n",
    "vecto= tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_words, output_mode='int', output_sequence_length=300)\n",
    "vecto.adapt(train['review'].to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5500254",
   "metadata": {},
   "source": [
    "We are now ready to define our model. Below, I first demonstrate a model with input and vectorization layer alone ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850f1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\langage-naturel-esiee\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   9,  226,    2, 1126,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building model : vectorization alone\n",
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(['I am the king'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c5a63",
   "metadata": {},
   "source": [
    "As we saw in labs 2 and 3, embeddings are mandatory. Hence, we will add an Embedding layer, but as opposed as what we did before, we will not initialize if from LSA, nor put it constant. Instead, we will let the model optimize this layer, possibly using dropout (if you use the related option). \n",
    "The dimension of 80 below is a crude estimation (barely from lab2 and results on LSA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4f07ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 80)           240160    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 240160 (938.12 KB)\n",
      "Trainable params: 240160 (938.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01085448,  0.00645819, -0.0082818 , ..., -0.01251129,\n",
       "         -0.01948293, -0.04546403],\n",
       "        [-0.04945812,  0.04329655,  0.02935851, ..., -0.02785053,\n",
       "          0.02604193,  0.00216408],\n",
       "        [ 0.03852944, -0.03609691, -0.04990917, ..., -0.01622276,\n",
       "          0.0064203 , -0.03805489],\n",
       "        ...,\n",
       "        [-0.00680554,  0.03858595, -0.01566784, ...,  0.0346531 ,\n",
       "         -0.02885078, -0.02837297],\n",
       "        [-0.00680554,  0.03858595, -0.01566784, ...,  0.0346531 ,\n",
       "         -0.02885078, -0.02837297],\n",
       "        [-0.00680554,  0.03858595, -0.01566784, ...,  0.0346531 ,\n",
       "         -0.02885078, -0.02837297]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vecto)\n",
    "model.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.predict(['I am the king'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc3488",
   "metadata": {},
   "source": [
    "Now it's up to you to devise and train two models which conforms those shown on slides 28 and 38 of Chapter 4 [here](https://perso.esiee.fr/~hilairex/AIC-5102B/rnn.pdf). Some pieces of advice :\n",
    "- Try first to reproduce the one on slide 28 using a SimpleRNN or LSTM. That one is the simplest.\n",
    "- Both have a return_sequence option, beware to what you are computing !\n",
    "- Remember that embedding turn integer indexes into vectors. Hence your input data is a sequence of *vectors* whatever type of RNN you use. Be careful to dimensionality and shapes.\n",
    "- In the end, you want a single scalar to represent a decision : yes or no (positive or negative)\n",
    "- Once training is done, you may try a predict() on thetest data, but such kind of simple (non stacked) RNN achieves an accuracy of about 82% at best (see Kaggle's benchmarks). \n",
    "- Keras has a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) and a [Concatenate](https://keras.io/api/layers/merging_layers/concatenate/) layers, which can be very handy. You may however build your model without using them, by using variables to connect the output(s) of a layer to the input of a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f3f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_slide_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 300, 80)           240160    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 300, 128)          26752     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 283553 (1.08 MB)\n",
      "Trainable params: 283553 (1.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 3s 63ms/step - loss: 0.7000 - accuracy: 0.5080 - val_loss: 0.6970 - val_accuracy: 0.4550\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.6914 - accuracy: 0.5200 - val_loss: 0.7497 - val_accuracy: 0.4550\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 2s 55ms/step - loss: 0.6956 - accuracy: 0.5330 - val_loss: 0.7182 - val_accuracy: 0.4550\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.6752 - accuracy: 0.5390 - val_loss: 0.6906 - val_accuracy: 0.5450\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.6507 - accuracy: 0.6420 - val_loss: 0.6875 - val_accuracy: 0.5450\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.6797 - accuracy: 0.5640 - val_loss: 0.6991 - val_accuracy: 0.4550\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 2s 48ms/step - loss: 0.6330 - accuracy: 0.6900 - val_loss: 0.6681 - val_accuracy: 0.5550\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 0.5259 - accuracy: 0.7690 - val_loss: 0.6561 - val_accuracy: 0.6300\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 1s 45ms/step - loss: 0.3524 - accuracy: 0.8640 - val_loss: 0.7095 - val_accuracy: 0.6200\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.2157 - accuracy: 0.9250 - val_loss: 0.8409 - val_accuracy: 0.6400\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.8409 - accuracy: 0.6400\n",
      "model_slide_28 Loss: 0.8409058451652527\n",
      "model_slide_28 Accuracy: 0.6399999856948853\n",
      "Model: \"model_slide_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (Text  (None, 300)               0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 300, 80)           240160    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 300, 256)          214016    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 487201 (1.86 MB)\n",
      "Trainable params: 487201 (1.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 13s 289ms/step - loss: 0.6930 - accuracy: 0.5040 - val_loss: 0.6904 - val_accuracy: 0.6550\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 9s 281ms/step - loss: 0.6662 - accuracy: 0.6560 - val_loss: 0.7402 - val_accuracy: 0.6100\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 0.6226 - accuracy: 0.6610 - val_loss: 0.6668 - val_accuracy: 0.7100\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 9s 288ms/step - loss: 0.4988 - accuracy: 0.7670 - val_loss: 0.5894 - val_accuracy: 0.7150\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 8s 263ms/step - loss: 0.4723 - accuracy: 0.8190 - val_loss: 0.6196 - val_accuracy: 0.6650\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 8s 260ms/step - loss: 0.2477 - accuracy: 0.9150 - val_loss: 0.7376 - val_accuracy: 0.6600\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 8s 246ms/step - loss: 0.2213 - accuracy: 0.9190 - val_loss: 0.6516 - val_accuracy: 0.7050\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 9s 272ms/step - loss: 0.1605 - accuracy: 0.9450 - val_loss: 0.8408 - val_accuracy: 0.7150\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 9s 274ms/step - loss: 0.1515 - accuracy: 0.9450 - val_loss: 0.5811 - val_accuracy: 0.7250\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 8s 266ms/step - loss: 0.0836 - accuracy: 0.9750 - val_loss: 0.8325 - val_accuracy: 0.7200\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.8325 - accuracy: 0.7200\n",
      "model_slide_38 Loss: 0.8325002193450928\n",
      "model_slide_38 Accuracy: 0.7200000286102295\n"
     ]
    }
   ],
   "source": [
    "train_labels = (train['sentiment'] == 'positive').astype(int)\n",
    "test_labels = (test['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# Slide 28\n",
    "model_slide_28_name = \"model_slide_28\"\n",
    "model_slide_28 = Sequential(name=model_slide_28_name)\n",
    "model_slide_28.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model_slide_28.add(vecto)\n",
    "model_slide_28.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "model_slide_28.add(tf.keras.layers.SimpleRNN(128, return_sequences=True))\n",
    "model_slide_28.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model_slide_28.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model_slide_28.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_slide_28.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_slide_28.summary()\n",
    "model_slide_28.fit(train['review'], train_labels, epochs=10, validation_data=(test['review'], test_labels))\n",
    "model_slide_28_results = model_slide_28.evaluate(test['review'], test_labels)\n",
    "print(model_slide_28_name, \"Loss:\", model_slide_28_results[0])\n",
    "print(model_slide_28_name, \"Accuracy:\", model_slide_28_results[1])\n",
    "\n",
    "# Slide 38\n",
    "model_slide_38_name = \"model_slide_38\"\n",
    "model_slide_38 = Sequential(name=model_slide_38_name)\n",
    "model_slide_38.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model_slide_38.add(vecto)\n",
    "model_slide_38.add(tf.keras.layers.Embedding(max_words+2, 80, input_length=seq_len))\n",
    "model_slide_38.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
    "model_slide_38.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "model_slide_38.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model_slide_38.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_slide_38.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_slide_38.summary()\n",
    "model_slide_38.fit(train['review'], train_labels, epochs=10, validation_data=(test['review'], test_labels))\n",
    "model_slide_38_results = model_slide_38.evaluate(test['review'], test_labels)\n",
    "print(model_slide_38_name, \"Loss:\", model_slide_38_results[0])\n",
    "print(model_slide_38_name, \"Accuracy:\", model_slide_38_results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0814b90",
   "metadata": {},
   "source": [
    "## Part B : inference in neural machine translation\n",
    "\n",
    "In this part, you will have to write a piece of code which will mimic the beam decoding algorithm shown on slides 30+ of [Chapter 5](https://perso.esiee.fr/~hilairex/AIC-5102B/lstm.pdf)\n",
    "\n",
    "The following code implements the network shown on slide 26, with the difference that inputs will not be words, but characters - this drastically reduces the memory requirements, to the price of a lower accuracy, however.\n",
    "\n",
    "The dataset are the french-english transcripts from the European parliament, which you should download - URL= https://www.statmt.org/europarl/v7/fr-en.tgz\n",
    "\n",
    "We will translate english sentences to french. We first load and sample the transcripts from local files. Note that the '\\</s\\>' special word on slide 26 has been replaced by a '\\x03' character to denote the end of a sentence. Likewise, the beginning of a sentence (which is missing in the decoder part, as it needs an input word or character) will be a '\\x02' special character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18905e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'europarl-v7.fr-en.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# data processing\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m english\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuroparl-v7.fr-en.en\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m french\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuroparl-v7.fr-en.fr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# begin and end special characters\u001b[39;00m\n",
      "File \u001b[1;32me:\\langage-naturel-esiee\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'europarl-v7.fr-en.en'"
     ]
    }
   ],
   "source": [
    "# https://www.statmt.org/europarl/\n",
    "\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# data processing\n",
    "english=open('europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "french=open('europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "# begin and end special characters\n",
    "begin='\\x02'\n",
    "end='\\x03'\n",
    "\n",
    "tran=[]\n",
    "i=0\n",
    "for x,y in zip(english,french):\n",
    "    if (len(x) > 0) and (len(x) < 30) and (len(y) > 0) and (len(y) < 40):\n",
    "        tran.append((x+end,begin+y+end))\n",
    "        i=i+1\n",
    "        \n",
    "\n",
    "# without sampling the above produces about 60k samples -> too much\n",
    "tran,_=train_test_split(tran,shuffle=True,train_size=20000)\n",
    "nsamples=len(tran) # about 60k samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368f6f7",
   "metadata": {},
   "source": [
    "We then build the vocabularies (=set of chars), and char->ord and ord->char dictionaries, for source (index=0) and target (index=1) languages. Those will be useful when vectorising sentences . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=[]\n",
    "char2num=[]\n",
    "num2char=[]\n",
    "maxlen=[]\n",
    "\n",
    "for lang in range(0,2):\n",
    "    voc.append(sorted(set([c for w in tran for c in w[lang]])))\n",
    "    c2n={}\n",
    "    n2c={}\n",
    "    for i in range(0,len(voc[lang])):\n",
    "        n2c[i]=voc[lang][i]\n",
    "        c2n[voc[lang][i]]=i\n",
    "    char2num.append(c2n)\n",
    "    num2char.append(n2c)\n",
    "    maxlen.append(max([len(w[lang]) for w in tran]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4a763",
   "metadata": {},
   "source": [
    "Next comes vectorisation : we replace every character directly by its one-hot binary representation. As a result, the vectorisation of a sentence is directly a tensor, and not a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorisation of sentences\n",
    "en=0\n",
    "fr=1\n",
    "    \n",
    "vecto=[]\n",
    "for lang in range(0,2):\n",
    "    vec=np.zeros((nsamples,maxlen[lang],len(voc[lang])), dtype='float32')\n",
    "    for sample in range(0,nsamples):\n",
    "        for row in range(0,len(tran[sample][lang])):\n",
    "            vec[sample,row,char2num[lang][tran[sample][lang][row]]]=1\n",
    "    vecto.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6dc60",
   "metadata": {},
   "source": [
    "Finally comes the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "# number of units to use in LSTM layers\n",
    "lstm_units=128\n",
    "\n",
    "# encoder side\n",
    "# input data = any string of the source language\n",
    "enc_input = keras.layers.Input(shape=(None, len(voc[0])))\n",
    "\n",
    "# transform this string by an LSTM layer\n",
    "[enc_out, enc_hidden, enc_cell] = keras.layers.LSTM(units=lstm_units, return_state=True)(enc_input)\n",
    "\n",
    "# decoder side\n",
    "# input is a translated string in the target language\n",
    "dec_input = keras.layers.Input(shape=(None,len(voc[1])))\n",
    "\n",
    "# the LSTM layer must return two vectors : the hidden state vector, and the cell vector\n",
    "# Must also return the full sequence, as the decoder is trained in teacher forcing mode\n",
    "dec_lstm = keras.layers.LSTM(units=128, return_state=True, return_sequences=True)\n",
    "[dec_out,dec_hidden,dec_cell] = dec_lstm(dec_input, initial_state=[enc_hidden,enc_cell])\n",
    "dec_output = keras.layers.Dense(units=len(voc[1]), activation='softmax', use_bias=True)(dec_out)\n",
    "\n",
    "# final model\n",
    "model= keras.Model(inputs=[enc_input, dec_input], outputs=dec_output, name='en2fr'+str(lstm_units))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94b543",
   "metadata": {},
   "source": [
    "The following trains the model from sampled data in teacher forcing mode.\n",
    "NOTE : epochs=5 is not enough at all, but performing a full train is *not* the aim of this part. What matters is the correctness of your code, not the result you will obtain - it will very likely resemble to a noisy string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587a077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# teacher forcing : expected output is the same than the decoded\n",
    "# sentence, except that it is shifted one time unit forward\n",
    "y= np.ndarray(shape=vecto[1].shape)\n",
    "y[0:nsamples-1,:,:]= vecto[1][1:nsamples,:,:]\n",
    "model.fit(x=[vecto[0],vecto[1]], y=y, validation_split=0.25, epochs=5, batch_size=64)\n",
    "#    saved_model='/home/shared/en2fra'+str(lstm_units)\n",
    "#    model.save(saved_model)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40988c",
   "metadata": {},
   "source": [
    "### Work to do : beam searching\n",
    "    \n",
    "Use the trained model below, including its final states, to write a piece of code which will execute a memoryless beam searching algorithm. This should do the following:\n",
    "1. Given an input string, encode it using the encoder model. That will give you a final hidden state (enc_hidden) and cell state (enc_cell)\n",
    "2. Set (enc_hidden,enc_cell) as the initial states of a decoder model, which should behave exactly as the one you built in the \"decoder side\" section, except that it has an initial state that must be set for any new input string\n",
    "3. Set the current character to '\\x02', to initially denote the beginning of the translated sentence \n",
    "4. If you feed the (vectorised) current character to the decoder, and ask for its prediction, you will obtain a probability distribution\n",
    "4. Following beam searching, from this probability distribution you should normally extract the $n$ most probable characters. We will simplify and choose $n=1$ (memoryless beam search) to keep the best candidate\n",
    "5. Add this best candidate to your decoded string, set the current character to this character, and loop to step 3 unless the decoded sentence is too long ($length > len(voc[1])$) or an '\\x03' character is predicted (end of sentence)\n",
    "\n",
    "Simply let your code produce its results. Don't expect good outputs, even though the model is properly built, there are issues with the data preparation, as explained in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3b8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
